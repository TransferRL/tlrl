\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Reinforcement Learning}{}% 2
\BOOKMARK [1][-]{section.3}{Transfer Learning for Reinforcement Learning}{}% 3
\BOOKMARK [1][-]{section.4}{Restricted Boltzmann Machines}{}% 4
\BOOKMARK [2][-]{subsection.4.1}{Bipartite RBMs}{section.4}% 5
\BOOKMARK [2][-]{subsection.4.2}{High-Order RBMs}{section.4}% 6
\BOOKMARK [1][-]{section.5}{Related Work}{}% 7
\BOOKMARK [2][-]{subsection.5.1}{Early Work}{section.5}% 8
\BOOKMARK [2][-]{subsection.5.2}{Contrasting Methods}{section.5}% 9
\BOOKMARK [2][-]{subsection.5.3}{Taylor's MASTER Algorithm}{section.5}% 10
\BOOKMARK [2][-]{subsection.5.4}{Ammar's TrRBM Method}{section.5}% 11
\BOOKMARK [1][-]{section.6}{New Extensions}{}% 12
\BOOKMARK [2][-]{subsection.6.1}{TrRBM for a Model-Free Setting}{section.6}% 13
\BOOKMARK [3][-]{subsubsection.6.1.1}{Using Source Q-Values Instead of Black-Box Target Task Rewards}{subsection.6.1}% 14
\BOOKMARK [3][-]{subsubsection.6.1.2}{Transferring Best- and Worst- Policy Instances}{subsection.6.1}% 15
\BOOKMARK [3][-]{subsubsection.6.1.3}{More Realistic Initial Sampling}{subsection.6.1}% 16
\BOOKMARK [1][-]{section.7}{Experimental Setup}{}% 17
\BOOKMARK [2][-]{subsection.7.1}{Environments}{section.7}% 18
\BOOKMARK [2][-]{subsection.7.2}{Untested Design Choices}{section.7}% 19
\BOOKMARK [1][-]{section.8}{Results}{}% 20
\BOOKMARK [1][-]{section.9}{Limitations}{}% 21
\BOOKMARK [1][-]{section.10}{Conclusions}{}% 22
